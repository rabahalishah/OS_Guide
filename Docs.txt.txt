*************Chapter 1**********************
****************Operating System**************

**kernal
The kernel is a computer program at the core of a computer's operating system and generally has complete control over everything in the system. The kernel is also responsible for preventing and mitigating conflicts between different processes.
Application <--> kernal<-->hardware

********Introdcution to OS***************
Operating system is simply a complete package of software which includes kernal and other system level component which helps a user to interact with the system hardware. In hardware we can have CPU, GPU, Storage devices, RAM and I/O devices.
The interaction with the hardware can be direct using power shell or cmd or it can be done via softwares. Such as printing a document using word file. Here the instrcutios (system call) were given by the software that is MS word to access the hardware that is Printer to print the document.
Operating system works on system calls such as read, open, write etc.

Example of OS: Windows, Mac, linux etc.

Goal of oS:
1) Convenience: The main goal of operating system is to provide convenience to the users. If there would be no Operating system then the user must have to write program to access the hardware. Which create complexities for the user to even perform a simple and easy task.
2) Throughput: Throughput means number of tasks perfoming per unit time. (Linux is famous for its high throughput)

Functionality of OS:
1) Resource management: Operating system is responsible for managing the resources. for example in case of server. Where thousands of users are accessing the same hardware. So in this case OS is responsible for managing the resources. OS will decide how much resources has to given to a user and once the user has done his task getting back the resources from the user.
2) Process Management: OS is also responsible for process management. For example you wrote a code in c and you want to execute its file. Then OS will schedule your execution in CPU (CPU Scheduling). Multi-tasking or Multi-Processing. If there are multiple tasks are going on then OS will schedule them to execute them in most efficient way.
3) Storage Management: (Hard drive) Concept of file system. (CISF common internet file system, Network file system). OS is also responsible for storage management. How th data will store in architecture of disk hardware that is also done by OS.
4) Memory Management: RAM. Memory management is also done by RAM. As we know that RAM is limited. So OS takes the responsiblity to load the program in RAM and make sure once the program has completed it must not be remain in the RAM. And other process which is waiting to load in the RAM should not wait if RAM is free. All allocation and deallocation is managed my OS. As we know program is first load in RAm and then given to the CPU
5) Security: The authentication process when we enter password. OS use kerberos security protocol. There are some task which can only done by admisitor. Do not allowing a normal user to perform admisitrative task is managed by OS. Moreover, as mentioned above when a process is load in RAM it is allocated with a portion of memory let say 0 to 100 and there is process 2 as p2 with memory allocation of 101 to 200 so in this case if any of the process will try to access or execute the instruction out way of its located memory slot then the process has blocked.

*************Types of Operating Systems
Batch
Multi-programmed
Multi-Tasking
Real-Time OS
Distributed
Clustered
Embedded

**Batch OS
These were very OS. In early ages of computer there were not PCs. Only big companies use Computers for big calculations etc. They offline store or load their programs in Punch Cards or you can say paper card or Magnetic tapes. Then they give it to the opertor. Then the operator insert this Punch card in Computer and then make batches. Batch is a set of instructions of similar job. Let say after inserting the puch card we have 3 batches and each batch contains diff. number of jobs. 1st batch is sent to the CPU and out that batch 1st job is sent to CPU for execution let say that job requires I/O device. CPU send it to I/O device for processing will wait until the process has completed during this time the CPU is Idle. He will not take the next job until the first one is executed. This was the major drawback of Batch OS.
With time refinments were made like they introduced monitors instead of using operators (employee) so can by yourself directory insert your punch card and do your job. Such as FORTRAN and IBSYS709X (In 1960)

**Multi-Programmed OS (Non-preemptive)
The concept was to load mutliple process in the RAM and then execute each path until its completion. Here the main focus was on idleness of CPU. CPU should not be idle. But the priority of system is to execute one process completely until unless the executing tasks contains instructions which is not CPU intensive such as I/O instruction etc. In that case CPU will allow the process to go to I/O device and in that time CPU will not remain idle. It will start executing the second task. The majore draw back here is that the last process has to wait. Means we lagged in response. But we successfully eliminated the idleness of CPU.

For examaple: 10 students each contains 5 questions to solve. Only one teacher which is acting as a CPU. All students are in the class. Here class is a RAM. One student will come to the teacher he will solve all 5 questions of it. then will move to the 2nd student. If student 1 wants to go to washroom or anyother task teacher (CPU) will not stop him to go. He will move to the student 2 and will start solving his 5 questions. and so on.

**Multi-Tasking/ Time sharing (Preemptive)
In such OS the CPU will give a specific amount of time to a certain task. If in that time the tasks has completely executed then well and good otherwise he will move to the 2nd task and so on. In this case CPU is not idle at all but here we have increased our response time. Now the last task will not have to wait much. He will get response fast.

For Example: 10 students each contains 5 questions to solve. Only one teacher which is acting as a CPU. All students are in the class. Here class is a RAM. One student will come to the teacher he will solve only 2 questions and will move to the 2nd student and so on. On reaching to the 10th student he will come back to the 1st student and will solve other 2 and so on. But in this case every student is getting response faster.

****Diff b/w preemptive and non-preemptive
Non-Preemptive scheduling in which Process transition is from ready state to running state (will execute completey until unless program it self do not go for other instructions)
Preemptive scheduling in which Process transition is from a running state to a ready state or from a waiting state to a ready state. 


**Real time operating systems
Those systems which respond immediately contains real time operating systems such as live streaming etc. There are two types of real-time operating systems such as soft OS and Hard OS. Soft Real time OS are used where time constraint is not strict such as live streaming. We can afford a little bit of delay in live streaming. But in case of defensive missiles we cannot afford delays in time. For such pursposes where time constraint is very important we use Real Time operating system.

***Distributed OS
Distributed refers to splitting a buisness into small sub-services. For example Meta distributed his database server geographically but they are connected to each other. To handle such distributed environments we use Distributed OS. They have advantage that if one system fails other can do its job.

***Cluster OS
Cluster refers to grouping of several server to make a big machine to deal a same business. They can be considered as one computer. For example supercomputer is a cluster. To deal with such systems we have Cluster OS.

***Embedded OS
OS for embbeded hardware that have only specific functionality. Such as embedded hardware of a home appliance. Microwave hardware have embedded OS only have one functionality to operate microwave oven. The embedded OS of micrwave cannot be used for embedded OS for a smart washing machine.


***********Process State
Its a very very important conceptual model for the understanding of user in operating systems
There are some primary and some secondary or you can say additional states

new ---> Ready ---> Running --->terminate

when we create a process and stores in a Hard drive that we can say its a stable process that will be in new state. Then the LTS (Long term scheduler) bring all the tasks in the Ready state (Its a state in RAM, concept of multi-programming). Then STS (Short term scheduler) brings one process from ready state to the Running state (The running state is in CPU, where processing happens). We are taking only one process at a time in Running state as we are assuming our whole OS syllabus is on Uni OS (means one CPU) not Multi-OS.
Once a process is completely executed in Running state it is send to the terminate state or you can say CPU dellocation. Now next task will come and so on. This is simple scenario of process state. 

**Case-1: (Preempitive)

new ---> Ready <---> Running --->terminate

Let say 1 process was running in the running state and then system calls a VIP process that has priority then the process which was running in the CPU (Running state) will send back to the Ready state (In RAM queue) and the VIP process will start executing in Running state.

or there could be a case that our CPU is running on 2 sec execution for each task. Then in this case the CPU will execute one task for 2 seconds and then will send back it to the Ready state and will execute 2nd process for 2 seconds and so on. All this transition between Running and ready state is done by STS (Short term scheduler)

** Case-2: (Non-Preempitive)

new ---> Ready ---> Running --->terminate
              \        /
               \      /
             Waiting/Block
 

In which task in the CPU (running state) will be execute completely until unless our process is to be processed by secondry device. Task do not come back to Ready state from running-state in case of Non-Preempitive scheduling. Here if the process is executing in the running state and the process itself requires a secondary device operation such as I/O device then in this case the process will be send in the waiting state. Here once the process is executed by the I/O device it will not be send back to running state but it will send back to the ready state then it will enter into the running state and then will be terminated. 

**NOTE: The difference in transition must be noted that in case of preempitive the process is directly send back to the ready state from Running state. But in case of Non-prempitive it is not directly send back to ready state but it is coming back from waiting state.

**Case-3: (Worst case, our waiting list get full)

new ---> Ready ---> Running --->terminate
              \        /
               \      /
             Waiting/Block (Full)
                  |
                  |
                Suspend 

let say we created 10 process and all process requires a secondary device for their execution then CPU will send all processes in the waiting state. Let say our waiting state get full (waiting state is also in RAM) then in this case MTS (Mid term scheduler) will send them in "Suspended State". Once there will be some space empty in the waiting list they will come in the waiting state and then again send back to Ready State (Remember once a task has entered into the waiting state or suspended it will not go back to running state but will send back to Ready state). Let say our tasks in the waiting list are taking too much time the process in the suspended state will not remain here forever but in such cases it will send back directly to the ready state.

new ---> Ready ---> Running --->terminate
         \     \        /
          \     \      /
           \  Waiting/Block (Full and taking too much time)
            \      |
             \     |
              <--Suspend 

**Case-4: (Ready State get full)

        Suspend
          |
          | (Full)
new ---> Ready ---> Running --->terminate
         \     \        /
          \     \      /
           \  Waiting/Block (Full and taking too much time)
            \      |
             \     |
              <--Suspend 

in such cases the process will send to Suspended state. Once there is some space in Ready state the process is entered in Ready state and so on.

Summarized and complete model

                Suspend
                   |
                   | 
new ---(LTS)---> Ready ---(STS)---> Running --->terminate
         \     \        /
          \     \      /
           \  Waiting/Block 
            \       |
             \    (MTS)
              \     |
               <--Suspend 

LTS: Long term scheduler
MTS: Mid term scheduler
STS: Short term scheduler

*****************Linux Commands
> ls (for list of directories)
> "ll" or "ls -l" (for detailed list of directories)
> sudo apt install package_name (use to install latest packages here apt means "Advance package tool manager" and sudo means "superuser do")

********Working with directories
> Man name_of_any_command (this command is used to see the manual of any thing that you will provide. e.g, "man cd" this command will give you the manual of cd command.
In linux a folder name start with "." will be considered as hidden file. It will be disappeared in GUI. If you want to see all you files whether they are hidden or not then you can use the following comman:
> ls -a (here -a flag is for all)
> ls -la (here -la is a combination of two flag that is length for l and all for a)
> ls -lah (here you will get all you files with long details and h for unit of their size like KB etc.)
> pwd (this command is used for checking the location where we are as it stands for print working directory)
> mkdir (this command is used for making directories)
> cd (This command is used to change the directory. This command requires an argument as name or path of directory where you want to go. If only cd alone is executed then it will bring you     to the home directory by default)
> mkdir -p folder1/folder2/folder3 (this command "mkdir -p" is used to create a tree nested folders in one line)
> rmdir (this command is used to remove the directory of which name the user have provided)
NOTE: we cannot delete a folder which is not empty
> rmdir -p folder1/folder2/folder3 (this command "rmdir -p" is used to delete nested folders in one line)

**********Working with files
// In linux files and folder names are case sensitive.
// In linux even a directory is considered as file. It is considered as special type of file. But yes, it is considered as a file.
> file (this command is used to check the type of the file which you have provided in the argument.
> touch file_name (this command is used to create empty files of which name is provided as an argument. NOTE: you can create more than one file by providing more than one names at once like >touch file1 file2 file3 (this will create 3 files))
> rm file_name (this command is used to delete the files. You can delete more than one file at a time)
> rm -i file_names (this commadn is to interact with the command while deleting. Like this will ask before deleting the file. For responsd "yes" for delete and "no" for ingore
> rm -rf (here -rf means recursive force. This command can be used to delete file or folder forcefully)
> cp file_name path newFileName (this command is used to copy files e.g > cp file1 /home/docs/ file1copied)
NOTE: If you want to copy the file within the same directory then you do not have to provide a path you simply have to provide a new name as you cannot copy or create files with the same name within the same directory but you can do it in other directory.
> cp -r folder_name path newFolderName (this command is used to copy directories. This will also copy the content which is in the directory)
> mv (this command is used to move and rename a file or folder. Syntax: >mv fileName path newFilename (this will move your file to the given path and will rename it at the same time.)
> mv fileName NewFileName (this command will only rename your file)

********************Working with file Content
> head file_name (this command is used to get the first 10 lines of the file)
> head -5 file_name (this command will onlly bring the first 5 lines. You add any number you want from 1 to 10)
> tail file_name (this command is used to get the last 10 lines of the files)
> tail -5 file_name (this command will onlly bring the last 5 lines. You add any number you want from 1 to 10)
> cat file_name (this command is used to print the full file. Here cat means concatenate. Here if you would print more than 1 file by giving each name in one line separated by spaces then it will print the contant of all files)
> echo your_content > fileName (this command is used to create files with the provided content. Here ">" this symbol is a part of the syntax)
> cat > fileName.extension (here cat command can also be used to create files and write content in it. After typing this command you will be get entered into writing mode. Write your content whatever you want and then to get exit of the writing mode press Ctrl + D )
> cat filename > newFileName (cat command can also be used to rename the file)
> more file_name (this command is also used to read the document. But this will print the file per page at a time. Like it will print let say 30% of the content then on pressing space button it will print the next 30% and so on)
> less file_name (this command is opposite of more.)

***********************System information commands
NOTE: Why use these commands? The answer is that when you will be working with the servers then there will not be any GUI then you have to do all things with commands.
> uptime (this command is used to check how long has been your system is running)
> free (this command is used to check how much your memory is free and how much your memory is in use)
> ps -A (tihs command is used to get the snapshot of current running processes)
> df (this command give you all info about your hard disk)
> sudo fdisk -l (this command is used to get the information about your disk partitions. Here sudo is used for administrative right. As this fdisk command cannot run without sudo)
> lsblk (this command is used to check the list of block devices)
> top (this command is used to display the linux processes in GUI format within the terminal)
> htop (this command is the upgraded version of top command and it will give you a better representation of linux processes. if this command is not installed by default then you can download it by using: > sudo apt install htop)
***************************Networking
> ifconfig (this command is used to get the network information of your system)
> ip or ip a (this command is used to show/manipulate routing, network devices.)

***************************Linux Package Manager
NOTE: There are some operations that linux restrict to their user to perform. In order to perform such actions sudo command is used which is used to execute the command as another user or super user
> sudo apt update (this command will tell you about the packages which are supposed to update. This command will only tell you. This will not update your packages)
> sudo apt upgrade (this command will upgrade your packages)
> sudo apt search package_name (this command is used to search the packages)
> sudo apt install package_name (this command is used to install the packages)
> sudo apt remove package_name (this command is used to uninstall the packages)

*********Text editors
There are several text editors in linux. While working with server there may be a need to edit files. There you will have to open then in any text editor of your choice with in the terminal and then use it.
we will learn 2 of them:
1. Nano
2. Vim

1) Nano:
> Nano file_name (this will simply open up the file in the terminal in insert mode. You simple have to write whatever you want and then press Ctrl+X. This will ask you to save or not. Y for yes and N for No.)
Now you can verify your changes by using >cat file_name

2)vim
>sudo apt install vim
>vim file_name (this will just open your file in vim editor in terminal in read mode. You wont be able to write anything as it is in read mode. To start writing you have to click "i" to enable --insert mode and then to save the changes you have to press escape first to get out of insert mode and then press 	colon ":" and write w so in short write :w and press enter.
to get exit from vim editor write :q
To get exit and save file in one command write :wq

************Some other commands
There three roles of a file
User, group and other (u, g, o)

And there are three position or conditions
Read, write and execute (r, w, x)

---     ---     ---
 u       g       o
rwx     rwx     rwx

We can make combinations and peform our desired action such as

>chmod r+w, r+x, x note
Means
User can read and write
Group can read and execute
Other can execute only

In terms of octal notation
r=6
w= 4
x=1

>chmod 555 note write and execute as w+x=4+1=5

Means ugo all 3 can

******************System calls
Whenever the user performs an action the action is done within the user mode. In order to perform kernal operations the user must have to use systm calls. For Example: we wrote program in C language that 2+2 = 4 here 2+2 will be considered as task in user mode. now in order to print of access the result that is 4 we have to use external libraries or system calls. Actually external library also make system calls at the end of the day. In C language the printf is a function which ultimately invoke system call to print the result on screen.
There are several type of system calls such as:

1) File Related: System calls to oper, read, write file we use open(), read(), write(), close()
2) Device Related: System calls for accessing a device such as I/O device to perform Read, writing, reposition opertions.
3) Information Related: System calls to get information about the system such as get PID, attributes, get system time
4) Process Control: System calls to control the process such as abort, fork (This creates multi tasking environment by creating childs of parent process where childs performing their own tasks and parents are performing their own tasks), load, wait
5) Process Communication: System calls where the communication b/w two process is required we use pipe() funtion etc. 

There are several system calls as in windows 10 we have 700 system calls. There can be other system calls related to security etc.

*******************FORK() System Call
This system call is used to create a child process of a parent process. Always remembers the child created by fork call is exactly a clone of parent process. There is a difference between fork and threads we will discuss them later on.
Here when we execute Fork() we either get two Ids one is 0 and other is +ve value. 0 for child and +ve value for parent process.

For example we created a program in C:

***
main(){

printf("Hello World!");
}

//Output: Hello World!

***

***
main(){

fork()
printf("Hello World!");
}

//Output: 
//Hello World!
//Hello World!


we will get two times execution as there are two process executing printf one is child and other is parent.

what if we use fork two times?

***
main(){
fork()
fork()
printf("Hello World!");
}

//Output: 
//Hello World!
//Hello World!
//Hello World!
//Hello World!


here the execution will be 4 times the reason is

		  p
		  |
   	        /   \
   1st fork()  /     \
              C1       P
             / \      / \
2nd fork()  /   \    /   \
          C2     C1 C3    P

at the end we have 4 processes. 3 Child and 1 parent so total 4 therefore, the execution will be 4 times
            
***3 times fork()
main(){
fork()
fork()
fork()
printf("Hello World!");
}

//Output: 
//Hello World!
//Hello World!
//Hello World!
//Hello World!
//Hello World!
//Hello World!
//Hello World!
//Hello World!

//8 times execution. 7 by child and 1 by parent
                        p
                        |
		    /       \
		   /         \
   	          /           \
   1st fork()    /             \
                C1              P
              /    \           / \
2nd fork()   /      \         /   \
            C2       C1       C3     P 
            / \      / \     / \    / \                
3rd fork() /   \    /   \   /   \  /   \
         C4     C2 C5    C1 C6  C3 C7   P          

here we have 7 childs and 1 parent therefore the execution was 8 times

Conclusion:

no of process = 2^n  (where n is the number of times fork command has used)
no of child process = (2^n)-1  ((where n is the number of times fork command has used)
no of parent process = 1 (this will always be 1)


Example question:
#include<studio.h>
#include<unistd.h>
int main(){
if(fork() && fork())
fork();
printf("hello world!");
}
  

output?

//output:
//hello world!
//hello world!
//hello world!
//hello world!


Explanation
we know that child process returns 0 and parent process returns a +ve value

so let say after if the first fork return a 0 so due to AND operator if condition would be false due to 0 so it will execute printf. So We will get 1 hello world.
Now for second case when the first fork returns +ve value then we will check for the second fork let say the second fork returns 0 then again we will get if condition false as 0 && 1 = 0 so this will again execute printf now we have total 2 hello world.
Now the 3rd case when then 1st and 2nd fork both return +ve number then if condition would become true then it will execute the 3rd fork which will create 2 process 1 child and 1 parent which is already present so these two processes will execute printf and we will get two more hello world so in total we will get 4 hello world.
(0 && 0) 1 hello world
(1 && 0) 1 hello world
(1 && 1) 2 hello world

total 5 hello world

Additonal info: In case of || OR operator we will get 5 hello world.
(0 OR 0) 1 hello world
(1 OR 0) 2 hello world
(0 OR 1) 2 hello world
(1 OR 1) 2 hello world


total 5 hello world

*************User mode VS kernel Mode
User Mode Vs Kernel Mode

In CPU there are generally two modes. One is User mode that is of 1 Bit and othe ris Kernel mode that is of 0 bit.

When a user execute a process such as opening and using application such as text editor then the processor will act as user mode. But when the user wants to access the operation that is hardware based such as accessing HD or I/O devices then system will generate a system call. Such as read() etc. Then the processor will shift the mode from user to kernel. Once the task has completed it will return to user mode.

The important point to note that why the processor is shifting mode. The reason is to manage the resource we cannot give all access to user to directly access all the resources there must be something in between that is OS and the mode that can perform such task is Kernel mode so we can say OS helps the processor to shift between user and kernel mode. 

2+2=4 in C language is a user mode task

But printf(2+2) will perform in kernel mode as it requires screen to display text.

Bank example: user go to the bank and to.access money he gives the cheque to the accountant here accountant is kernel who have access to the locker. Account will grap money from locker and give to the user.

**************Process VS Threads
It would be better to say MultiProcessing vs MultiThreading

Process are generally termed as heavy processes. 
They are invoke by system calls. Such as fork().
System calls is must in processes.
When a system call is made. CPU creates a clone of the process. Means each clone would have its owm stack, register, data and code. There will be multiple copies. This clone is also refred as child process. 
Parent and child components are completely independent. Blocking one process will not effect the blocking of chlid process (blocking means accessing HD or I/O devices etc.)
Context switching is very slow. (As we kmow in multiprocessing we run a process for a certain period of time and then switched to the other process and so on) here while switching the values must be store in PCB (Process control Block) for the process one before shifting to the second process as in process we have completely independent clones. 

Threads:
Threads are termed as user level process means kernel have no idea about them. Application is responsible for creating threads not kernel.
As theh are user level operation so they do not involves any system calls.
In threads clones are not created. Only threads are created where they share same data and code but for the same data and code we have multple threads which have their own register and stack.
Since they are sharing data not creating copies to switching context is very fast.
Drawback, one thread is block the whole process will.be blocked. The reason is that kernal consider them always as one process no matter how many threads are there. Therefore blocking one thread will block the whole process.

Processes are dependent in threads.

*************User level threads VS kernel level threads
1. User level thread Vs Kernal level thread
2. User level threads are the threads that are managed by the user or you can say managed by application
3. These threads share data and code. Each thread have its own separate register and stack.
4. Context sharing is very fast as data and code are same

Kernel lvl threads

1. These are the threads which are managed by OS.
2. They also share data and code
3. But they are slower in context switching as compared to user level threads.
4. They are independent threads as process is managed by the OS so it do not block when one thread is blocked.

You can simply assume kernel lvl threads as process.

Time taken by the process:

Process context switiching time > kernel lvl threads context switching time > user lvl threads context switching time.


*************Chapter 2**********************
***************Process Scheduling Algorithms (Or CPU scheduling)
There are two terms that you must be familiar with before discussing scheduling algorithm
1. Premptive (The process in the running state can be send back to ready state to execute it in future. Concept of multiprocessing)
2. Non-premptive (The process in the running state do not send back to ready state. It will be completed unless the task itself requires a system call such as accessing I/O device.)

Premptive 				|		Non-Premptive
1. SRTF (Shortest remaining time first)	|	 FCFS (First come First Serve)
2. LRTF (Longest remaining time first)	|	 SJF (Shortest Job First)
3. Round Robin				| 	 LJF (Longest Job First)
4. Priority Based			| 	 HRRN (Highest Response Ratio next)
					| 	 Multilevel Queue
					|	 Priority Based


**************Important terminologies in process scheduling
Arrival Time: The time at which the process enters into the ready state.
Burst Time (Duration): The time taken by the process to get executed in CPU.
Completion time: The time at which the process execution get completed.
Turn around time (Duration): it is the total time taken  by the process from ready state to its completion, Formula =  {Completion Time - Arrival Time}
waiting time (Duration): The time taken by the process to wait. Idle time. Formula = {Turnout Time - Burst Time}
Respomse Time: The time at which the process get a response {The time at which process get the cpu for the first time - Arrival Time}


Example: Let say a person enters a bank at 11am and get out from the bank at 12pm. He took only 15mins to deposite his payment. Rest of the time he had to wait in a queue.
Here,
Arrival Time (AT) = 11am
Burst Time (BT): 15 mins
Completion time (CT)= 12pm
Turn around time (TAT) = {12pm - 11am} = 1hr or 60 mins
Waiting time (WT) = {60 mins - 15mins} = 45 mins
Response time (RT)= {11:45 - 11am} = 45mins

**NOTE**: In case of non-premptive Wait time and response time would be same.


**Context Switching**:  means that saving the values of current process in PCB (Process control block) and sending it back to the ready state and add the new process. Why we are saving the current running process in PCB? The answer is that when we again run it in the future so we do not have to run it from the start we just have to resume it from where we have pause it. PCB will help us in doing it so. As PCB carries all the information about the process.

************************Example Problem FCFS Algorith (non-premptive):
Steps to follow:
Criteria will be arrival time. Means we will take decisions on the basis of arrival time.
1) FCFS with non-premptive is very easy. 
2) Simply make a gant chart starting with 0 arrival time.
3) Simply add the processes on the basis of their arrival time and run the process complete once the process has added.
4) The would be no clash in FCFS. If any then the one who came first would be executed no matter how long its burst time is.
5) Add process, completely execute it till its burst time and then again check what next task is coming on the completion of previous task if yes then add it otherwise idle the processor.
(This is draw back of FCFS algo as cpu remains idle at some points)

Q: Data:

Process no.	| Arrival Time  | Burst Time
p1		| 0		| 2
p2		| 1		| 2
p3		| 5		| 3
p4		| 6		| 4

Calculate: Completion Time, Turn around Time, Wait time, Response Time, Avg. TAT and Avg. WT = ?
Mode: Non-premptive
Criteria: Arrival Time
Algorithm: FCFS (firt come first serve)

Solution:
The best way to do it make a Gant chart: 

 P1             P2  N/A  P3   p4
_____________________________________________
0              2   4    5    8    12 
^              ^
Time           Time at which
at which       p1 completed and
the process    p2 got the CPU
get the CPU



TABLE:
Process no.	| Arrival Time  | Burst Time    | TAT		| Wait Time	| Response Time
p1		| 0		| 2		| 2-0 = 2	| 2-2= 0	| 0-0 = 0
p2		| 1		| 2		| 4-1 = 3	| 3-2= 1	| 2-1 = 1
p3		| 5		| 3		| 8-5 = 3	| 3-3 = 0	| 5-5 = 0
p4		| 6		| 4		| 12-6 = 6	| 6-4 = 2	| 8-5 = 2


Avg. TAT = (2+3+3+6)/4 = 14/4 = 3.5
Avg. WT = (0+1+0+2)/4 = 0.75

P1 arrived at 0 and get the CPU at 0 and it has BT of 2 so it completed at 0 + 2 = 2
During 0 to 2. The P1 had arrived in the ready state or RAM at 1 so next it will be executed. So after waiting for 1 P2 get the CPU at 2 and it has BT of 2 so it completed at 4.
During 2 to 4 none of the process arrived only p2 get executed. Then at 5 p3 came into the memory and get the CPU at 5 and has BT of 3 so it competed at 8.
DUring 5 to 8 p4 has arrived in the ready state at 6 and has to wait 2 and got the CPU at 8 and it has BT of 4 so it completed at 12.





**************************************Example Problem SJF Algorithm (non-premptive):
Steps to follow
Here criteria would be burst time. Means we will take decisions on the basis of burst time.
1) SJF with non-premptive is very easy
2) Make a gant chart starting with 0
3) Add the process which have shortest burst time and run it completely.
4) Once it ran completely check for other task which are coming on its completion or during its execution.
5) Compare the burst time of coming task with the task who already have arrived in ready state. pick the shortest one and completely execute it.

Q: Data:
Process no.	| Arrival Time  | Burst Time
p1		| 1		| 3
p2		| 2		| 4
p3		| 1		| 2
p4		| 4		| 4

Calculate: Completion Time, Turn around Time, Wait time, Response Time= ?
Mode: Non-premptive 
Criteria: Burst Time
Algorithm: SJF (Shortest Job first)

Solution:
The best way to do it make a Gant chart: 

 N/A   P3   P1    p2   P4
_____________________________________________
0   1     3    6     10  14




TABLE:
Process no.	| Arrival Time  | Burst Time    | TAT		| Wait Time	| Response Time
p1		| 1		| 3		| 6-1 = 5	| 5-3= 2	| 3-1 = 2
p2		| 2		| 4		| 10-2 = 8	| 8-4= 4	| 6-2 = 4
p3		| 1		| 2		| 3-1 = 2	| 2-2 = 0	| 1-1 = 0
p4		| 4		| 4		| 14-4 = 10	| 10-4 = 6	| 10-4 = 6


Here the concept is that the process having shortest burst (execution) time will be executed first. Since its non-premptive response time would be same as wait time.
P1 and P3 both arrives at 1 but p3 will be executed first as it has shorter burst time. It will take 2 burst unit time so it will completed at 3 unit time. During 1 to 3, P2 comes and p1 is already in ready state. now we will have to decide between p2 and p1. Since p1 have short burst time p1 will be executed first and will be completed at 6. Now till 6 p2 is already in ready state and p4 has came at 4. Now here p2 and p4 both have same burst time. So which would get execute first? The one who arrived 1st. So the arrival time of p2 is less than the arrival time of p4 so we will go with p2 and then p4.


**IMPORTANT NOTE**: Always look for arrival time first. In case there are more than one process arrived at the same time than you have look for other factors such as burst time etc. Depends on the algorithm you are using. And in case of non-premptive Once the task has started executing it will be completely executed till its burst time. 




**************************************Example Problem SRTF Algorithm (or SJF with premptive):
Steps to follow:
Here criteria would be burst time. Means we will take decisions on the basis of burst time. But we will not completely run the process at once as it is premptive. 
1) SRTF or SJF with premptive is tricky
2) Make a gant chart starting with 0
3) Check who have arrived at 0. If yes, then run it only for 1 unit time.
4) After one unit update the burst time of process 1 and check which next process is coming.
5) Compare the values of coming process with the one who already is in running state. If the coming process have short burst time than the one which is already in the running state then   switch the context and add the coming one and send back the running process into the ready state. And so on.
7) In short run 1 unit time and compare values, pick the shortest one and update burst time

Q: Data:
Process no.	| Arrival Time  | Burst Time
p1		| 0		| 5
p2		| 1		| 3
p3		| 2		| 4
p4		| 4		| 1

Calculate: Completion Time, Turn around Time, Wait time, Response Time= ?
Mode: premptive 
Criteria: Burst Time
Algorithm: SRTF (SJF with premptive)

Solution:
The best way to do it make a Gant chart: 
Process no.	| Arrival Time  | Burst Time
p1		| 0		| 5 --> 4 --> 3 --> 2 --> 1 --> 0
p2		| 1		| 3 --> 2 --> 1 --> 0
p3		| 2		| 4 --> 3 --> 2 --> 1 --> 0
p4		| 4		| 1 --> 0
 
  p1  p2   p2   p2   p4  p1  p1  p1  p1  p3   p3  p3  p3
__________________________________________________________
0   1    2   3    4   5   6    7   8   9    10  11  12  13

In case of SRTF we execute each process for only 1 unit and then compare its remaining burst time with rest of the processes who have arrived during that interval. Since it premptive so process can be send back to its ready state from running state after getting 1 time unit executed. After each time execution the process will be compared with the burst time of the process in ready state and then the one having shortest will executed 1 unit time and its is compare with rest of the processes with their updated burst time and so on.


TABLE:
Process no.	| Arrival Time  | Burst Time    |Completion time| TAT		| Wait Time	| Response Time
p1		| 0		| 5		|     9		| 9-0 = 9	| 9-5= 4	| 0-0 = 0
p2		| 1		| 3		|     4		| 4-1 = 3	| 3-3= 0	| 1-1 = 0
p3		| 2		| 4		|    13		| 13-2 = 11	| 11-4 = 7	| 9-2 = 7
p4		| 4		| 1		|     5		| 5-4 = 1	| 1-1 = 0	| 4-4 = 0


***************Round Robin (premptive)
Steps to follow:
Here criteria would be Time Quantum. We will run the process only for given time quantum unit and the switch the context.
1) Round Robin is very tricky
2) Make two gant charts one will for ready state and other for running state. we will use only running state gant chart for calculations but in Round robin due to context switching we will have to make ready state queue
3) Ok so first add a process in ready queue at 0 and then add it in the running state. 
4) Run this process for only till the given time quantum unit. And then check how many process has already come in the ready state till its completion and check after updating its burst time whether the current process has completed or not? If yes then add it back in ready state (switch the context). While switching always remember that the process which is going back to the ready state would come at the end of process which are already in ready state. Now update their burst time. And pick the one which is next in the ready no matter how long their burst time is. Always fill ready state according to the arrival time of processes.
5) Again run it till the given time quantum and then repeat the process.


Q: Data:
Process no.	| Arrival Time  | Burst Time
p1		| 0		| 5
p2		| 1		| 4
p3		| 2		| 2
p4		| 4		| 1

Given Time Quantum: 2
Calculate: Completion Time, Turn around Time, Wait time, Response Time, no. of context switching= ?
Mode: premptive 
Criteria: Burst Time
Algorithm: ROund Robin

Solution:
The best way to do it make a Gant chart: 
Process no.	| Arrival Time  | Burst Time
p1		| 0		| 5 --> 3 --> 1 --> 0
p2		| 1		| 4 --> 2 --> 0
p3		| 2		| 2 --> 0
p4		| 4		| 1 --> 0 
 
	  	 p1 p2 p3 p1 p4 p2 p1
Ready State:	__________________________________________________________
		0     2       4  

	  	 p1 | p2 | p3 | p1 | p4 | p2 | p1
Running State:	__________________________________________________________
		0   2   4     6    8    9    11   12

"|" = context switched. 
count "|" to check the no. of context switching.

NOTE: here we run our p4 only 1 unit but the given time quantum was 2. So its fine. The reason is that p4 only have 1 burst time. Time quantum is applicable only for those process which have burst time greater than the given time quantum.

TABLE:
Process no.	| Arrival Time  | Burst Time    |Completion time| TAT		| Wait Time	| Response Time
p1		| 0		| 5		|    12		| 12-0 = 12	| 12-5= 7	| 0-0 = 0
p2		| 1		| 4		|    11		| 11-1 = 10	| 10-4= 6	| 2-1 = 1
p3		| 2		| 2		|     6		| 6-2 = 4	| 4-2 = 2	| 4-2 = 2
p4		| 4		| 1		|     9		| 9-4 = 5	| 5-1 = 4	| 8-4 = 4

No. of context switing = 6
**Context Switching**:  means that saving the values of current process in PCB (Process control block) and sending it back to the ready state and add the new process. Why we are saving the current running process in PCB? The answer is that when we again run it in the future so we do not have to run it from the start we just have to resume it from where we have pause it. PCB will help us in doing it so. As PCB carries all the information about the process. In short transition from p1 to p2 on running state queue.



**************************************Example Problem Priority Scheduling Algorithm (premptive):
Steps to follow:
Here criteria would be Priority number. Means we will take decisions on the basis of given pririty number for each process. But we will not completely run the process at once as it is premptive. 
1) Priority with premptive is easy
2) Make a gant chart starting with 0
3) Check who have arrived at 0. If yes, then run it only for 1 unit time.
4) After one unit update the burst time of process 1 and check which next process is coming.
5) Compare the priority value of the coming process with the one who is already in running state. If the coming process have greater priority than the one which is already in the running state then switch the context and add the coming one and send back the running process into the ready state. And so on.
7) In short run 1 unit time and compare priority values, pick the higher priority one and update burst time.

Q: Data:
Priority No. |	Process no.	| Arrival Time  | Burst Time
10	     |	p1		| 0		| 5
20	     |	p2		| 1		| 4
30	     |	p3		| 2		| 2
40	     |	p4		| 4		| 1

Calculate: Completion Time, Turn around Time, Wait time, Response Time= ?
Mode: premptive 
Criteria: Burst Time
Algorithm: Priority

Solution:
The best way to do it make a Gant chart: 
Priority No. |	Process no.	| Arrival Time  | Burst Time
10	     |	p1		| 0		| 5 --> 4 -->0
20	     |	p2		| 1		| 4 --> 3 --> 0
30	     |	p3		| 2		| 2 --> 1 --> 0
40	     |	p4		| 4		| 1 --> 0
 
  p1  p2   p3    p3   p4    p2   p1
__________________________________________________________
0   1    2    3     4    5     8   12

Priority No. | Process no.	| Arrival Time  | Burst Time    |Completion time| TAT		| Wait Time	| Response Time
10	     | p1		| 0		| 5		|    12		| 12-0 = 12	| 12-5= 7	| 0-0 = 0
20	     | p2		| 1		| 4		|    8		| 8-1 = 7	| 7-4= 3	| 1-1 = 0
30	     | p3		| 2		| 2		|    4		| 4-2 = 2	| 2-2 = 0	| 2-2 = 0
40	     | p4		| 4		| 1		|    5 		| 5-4 = 1	| 1-1 = 0	| 4-4 = 0


NOTE: In case we have two process having same priority then go for their arrival time. FCFS. If arrival time is also same then go with their ID like p1 first then p2 and so on.
Higher the priority number, Higher will be the priority.

**************************************Example Problem Priority Scheduling Algorithm (premptive) with mix burst time:
Steps to follow:
Here criteria would be Priority number. Means we will take decisions on the basis of given pririty number for each process. But we will not completely run the process at once as it is premptive. 
1) Priority with premptive is tricky
2) Make a gant chart starting with 0
3) Check who have arrived at 0. If yes, then run it only for 1 unit time.
4) After one unit update the burst time of process 1 and check which next process is coming.
5) Compare the priority value of the coming process with the one who is already in running state. If the coming process have greater priority than the one which is already in the running state then switch the context and add the coming one and send back the running process into the ready state. And so on.
7) In short run 1 unit time and compare priority values, pick the higher priority one and update burst time.
8) also here I/O time would be given during the I/O time of one process other process can be executed depending upon their arrival and priority.

Q: Data:					  Burst Time
Priority No. |	Process no.	| Arrival Time  | CPU   I/O   CPU
2	     |	p1		| 0		| 1      5     3     
3	     |	p2		| 2		| 3      3     1
1	     |	p3		| 3		| 2      3     1
4	     |	p4		| 3		| 2      4     1

Calculate: Completion Time of p1, p2, p3, p4 and CPU Idleness ratio and CPU usage ratio = ?
Mode: premptive 
Criteria: Burst Time
Algorithm: Priority
Here lower the number higher the priority (Given)

Solution:
The best way to do it make a Gant chart: 
Priority No. |	Process no.	| Arrival Time  | CPU      	   I/O         CPU
2	     |	p1		| 0		| 1-->0           5-->0     3-->2--1-->0     
3	     |	p2		| 2		| 3-->2-->1-->0   3-->0     1-->0
1	     |	p3		| 3		| 2-->1-->0       3-->0     1-->0
4	     |	p4		| 3		| 2-->1-->0       4-->0     1-->0
 
                        <----I/O of P3-->             <----I/O of p2--->
  p1  N/A  p2   p3   p3   p2   p1   p1   p3   p1   p2   p4    p4   N/A   p2    N/A N/A  p4
___________________________________________________________________________________________
0   1    2    3    4    5    6    7    8    9    10   11    12   13   14     15   16  17   18
    <-------I/O of P1-------->	                                  <-------I/O of p4--->

Priority No. |	Process no.	| Arrival Time  | CPU      	   I/O         CPU 	  | Completion Time
2	     |	p1		| 0		| 1-->0           5-->0     3-->2--1-->0  |   10
3	     |	p2		| 2		| 3-->2-->1-->0   3-->0     1-->0	  |   15
1	     |	p3		| 3		| 2-->1-->0       3-->0     1-->0	  |   9
4	     |	p4		| 3		| 2-->1-->0       4-->0     1-->0	  |   18

CPU Idleness ratio = (no. of idle units)/total units
So
CPU Idleness ratio = 4/18 = 0.22 

CPU usage ratio = (no. of usage units)/total units
CPU usage ratio = 14/18 = 0.77

NOTE: In case we have two process having same priority then go for their arrival time. FCFS. If arrival time is also same then go with their ID like p1 first then p2 and so on.
Higher the priority number, Higher will be the priority.


***************Multi-level queue
Here the concept is that when we have different nature of process then why there is only one ready state for all these process. They should have their own ready queue and they should be executed on the basis of best suitable algorithm.

Highest priority --> System processes (such as inturption etc.) ----------SJF--------------|
											   |			
Medium priority --> Inerative processes (such as application like VS code etc.)--SRTF---->CPU
                                                                                           ^
											   |
lowest Priority --> Batch process (Background processes)--------------------FCFS------------


Drawback: The problem here is that if there are alot of system processes then interactive processes and batch process will have to wait long for their turn they would enter into starvation zone. To solve this problem we use multi-level feedback queue.
here **starvation** means that when highest priority process are coming one by one due to which low priority processes are not getting their turn to execute.

**************Multi-level feedback queue
Here the concept is that the lower priority process will send a feedback to change its priority. So we change its priority or change its ready queue with time so that it can get some execution instead of going into starvation

************************Chapter 3
*****************Process synchronization, Process types, race conditions

Types of process
There are two types of process
1) Cooperative processes (Those process which do share something such as variable, memory, code or resource such as I/O devices are called cooperative processes. Here execution of one process can affect the output of other process)
2) Independent Processes (Those process which do not have anything common in between them they are totally independent. The execution of one process will not affect the execution of other process)


Q: Why do we need process synchronization? 
The answer of this question can be understand very clearly by seeing an example. But just do remember in case of cooperative processes where processes are sharing resources, process synchronization is very crucial as it can create a problem. Let see how:

P1			| P2
int shared = 5; 	| int shared = 5;      
int x = shared;		| int y = shared;
x++;			| y--;
sleep(1);		| sleep(1);	
shared = x; 		| shared = y;

Let say we have these two cooperative processes as they are sharing a single variable called "shared". We ran the process P1 first.

//In P1
// initialized share as 5
// assign 5 to x
// incrementing the value of x.
//now x = 6
// pausing the process for 1 sec using sleep(1) command

Here will the CPU will be idle? NO, The CPU will switch the context. Means it will save the information of process 1 in PCB (process code block) and will start executing the process P2

//In P2
// initialized share as 5
// assign 5 to x
// decrementing the value of x.
//now x = 4
// pausing the process for 1 sec using sleep(1) command

Now CPU will again switched the context to P1
(Resuming p1)
//In p1
// now shared = 6
//terminate

Now CPU will terminate the process 1 and will move to p2
(Resuming P2)
//In p2
// now shared = 4
//terminate


We will get either 6 or 4.
What do you think. Is this true?
NOOO!!! The reason is that we had common variable. We added 1 in X and then subtracted 1 from X we should have the same answer but we got two different answers which were wrong. This condition is called race condition where two answers are in race to win. Although both are wrong.
So in order to avoid such race conditions for cooperative processes the process synchronization is very important.


********************Producer and consumer example of process synchronization
//Consumer Code				| //Producer code
void consumer(){			| int count = 0;
int itemC;				| void produce(void){
while(true)				| int itemP;	
{					| while (true){
while(count == 0);			| produce.item(itemP)
itemC = Buffer(out);			| while(Count == n)
out = (out+1)mode(n);			| Buffer[in] = itemP
count = count -1			| in = (in + 1) modn;
process.item(itemC) 			| count = count + 1;
}					| }
}					| }



assume buffer as a space in memory. The logic here is simple. The producer code produces or add an item in the buffer. And consumer code extract, remove or consumer that item. If producer code runs first and then consumer code we will get every thing fine. But the problem will create where when there might be interuption in incrementing the count variable.

let say the producer code ran the code for 3 times. Means it has added 3 items in the buffer. At index 0, 1 and 2. When producer code tried to add 4th item at index 3 in the buffer when it reaches the line "count = count + 1". here CPU runs the instructions by dividing it into microinstructions such as for count= count +1 ---> 
I1. current count value will be loaded in Register, (CPU uses register to increase its efficiency) (here current value is 3 as there are 3 items in the buffer)
I2. Current value will be incremented.
I3. update value will be assigned to count variable.

assume that by any means in above three steps the execution prempt before point 3. Means the value has incremented to 4 but not assigned and 4th item has added in the list. All the information will be stored in PCB and context will be switched to the consumer code. The consumer code will remove the first item. And count will be updated to 2 as one item has been removed out of 3. And again when the CPU reaches at 
"count = count -1"
I1. current count value will be loaded in Register, (here current value is 3 but there are 4 items in the buffer. Current value could not update due to premption in producer code.)
I2. Current value will be incremented.
I3. update value will be assigned to count variable.
and before reaches point 3 the code has prempt. Now here the value of count has decremented to 2 but not assigned. All info will be saved PCB and context will be switched. 
Now consumer process will be resume.

Recall: we left producer code with count value incremented to 4 but not assigned and at that time there were 4 items in the buffer. Now count value will be assigned to 4 to complete the producer code. 
wait wait wait!!! How many items are there in buffer now? The answer is 3 and what our count is showing due to producer code is 4.
Now once this process will be terminated the consumer code will resume. Where the count value has been decremented to 2 and now it will be assigned to count and process will be terminated. Now count will have value of 2 but there are 3 items in buffer list.
Again we have reached the race condition. Dont forget that these processes are cooperative process as they are sharing a common variable count. Since there were no process synchronization between them therefore, This race condition happened.

So the process flow which can create race condition is: producer I1, I2, Consumer, I1, I2, Producer I3, consumer I3

***********************Printer-spooler problem | process synchronization
I1. Load Ri , m[in]
I2. store SD[Ri], "File_name"
I3. Increment Ri
I4. Store m[in], Ri


This is a spooler code. Printer is a very slow device its take instructions one by one and execute one by one. Printer before printing the documents stores them in a spooler directory.
It simply load the index in register and assign it to Ri (register index), Then take the file name and store it into the spooler directory at the Ri and then after storing, It increment the Ri to place the next coming file in next index.

Let assume there are 4 files stored in spooler directory. let say there come two process at the same time p1 and p2 having file name f5.doc and f6.doc. 
Process 1 ran first.
// In spooler code for process 1
Currently there are 4 files at index, 0 1 2 3 and current value for index is 4 as 5th place is empty.
Index 4 will be loaded and f5.doc will be stored at the 4 index in spooler directory.
Now Ri has incremented as in 3 line of spooler code. Before assigning the incremented index that is 5 the process 1 prempt. All information get stored in PCB and context get switched,
Now process 2 will start executing. Remember the Ri is 4 as it was only incremented but not assigned so the index has not updated. here the file6.doc will also be stored in the 4th index as it was not updated. So it means f5.doc will be overwrite and f6.doc will be store. now the Ri incremented to 4 and stored in I4 line of p2 process and p2 will be terminated.
Now p1 will be resumed and it assign the Ri index to 5 and will terminated.


here the big and serious problem has come that is Lose of data. We didnot implemented process synchronization and this happened.

The process followed which create this lose of data condition is: P1, I1, I2, I3, prempt, P2, I1, I2, I3, I4 , P1 I4


**************Critical Section and process synchornization mechanism
When there are two process and they have a common section of code which can effect the each other processes is called critical section. In general every program have two section. One is independent and other is critical section. 

In order to prevent race conditions and loss of data we implement process synchronization.
Every process synchronzation mechanism should follow 4 rules. Process sychnronization mechanism is nothing just a piece of code which we write at entry point and exit point of critical section.

Primary:
1) Mutual Exclusion: Means if one process is enter in critical condition then p2 will not be allowed to enter into critical section and vice versa. We write a code at the entry section of critical section. If one process is in the critical section the entry section of p2 process wont allow it to enter the critical section until p1 get terminated. This should be followed.
2) Progress: This is a condition in which code of one program do not allow the other process to enter into the critical section. Neither that process is going to critical section nor it is allowing other processes to get enter. This should not be the case in you process synchronization mechanism or code.

Secondary:
3) Bounded wait: Let say p1 enters into the critical section and then get out and then again get enter into the critical section and so on. It do not allowing the turn of process 2. Process 2 will remain waited for ever. This should be the case in you process synchronization mechanism or code.
4) No assumption related to hardware: Your code should not be dependent on the hardware such as it will work on linux and wont work on windows. It should not be the case.


*****************Implementation of process synchronization

***********Method 1 LOCK variable***************:
This is executed in user mode.
Used for multiprocess solution.
This is one approach to implement process synchronization. 

LOCK = 0 means critical section is free
LOCK = 1 means critical section is not free

1) while(LOCK == 1);
2) LOCK = 1;
3) Critical section
4) LOCK = 0;

Case 1:
Initiallay Lock = 0

P1 comes while condition false as LOCK = 0
LOCK value set to 1
critical section

now p1 is in critical section and value of LOCK variable is 1

when P2 will come while condition will become true and since there is a semi colon after while loop means p2 will struck in the infinite loop and will not be allowed to get enter into the critical section until p1 do not get out.

when p1 will get out it will execute the line 4 and lock will set to 0 and now p2 will be allowed to come.


**IMPORTANT** DO you guaranttee that mutual exclusion will always be followed in every case of Lock variable? The answer is NO.

Case 2:
Let say

p1 comes and initially LOCK = 0 so while loop will be false
as when the p1 was about to execute the 2nd line that is LOCK = 1; before executing it p1 get prempt
during this time p2 comes while(LOCK ==1) becomes false and LOCK will be set to 1  p2 will be allowed to get enter into the critical section.
Now p1 come back and resume from where it left. P1 will resume from line 2 where LOCK will be again set to 1 which is already 1. and p1 will also be allowed to get into critical section.

Now both processes are in critical section. Mutual exlusion failed in this case of lock variable.
so no guaranttee for mutual exclusion


**************Method 2 "Test_and_Set" instruction

we simply resolved the upper case by combining the two instructions in one instruction.

Since the premption was possible between these two lines:
1) while(LOCK == 1);
2) LOCK = 1;
3) critical section
4) LOCK =0;

so we combined both and make it a single instruction as

1) while(test_and_set(& lock));
2) critcal section
3) lock = false;

//here test_and_set() function is doing both testing the incoming value and setting the lock value both in one command:

//test_and_set(boolean *traget){
boolean r = *target
*target = TRUE;
return r;

}


so let say p1 process came who have initially the value of lock= false
here *target is a pointer which is pointing toward the value of lock in the memory at some address. here we are calling by reference thats why we have used &lock.
okay so in simple initially the value of lock was false. which is assigned to r and returned. So r = false will be returned so while condition will be falsed and p1 will get enter into the critical section. Before returinng the vakue of r. By using *target = TRUE. we have set the value of LOCK = TRUE as *target is pointing toward the value stored in LOCK variable. 

now if the process 2 comes. now the value of lock is TRUE so when the test_and_set function will be called, here due to "boolean r = *target" the value of r will be set to the value which is stored in LOCK variable (AS *target is pointing toward the value stored in LOCK variable). So r= true and it will returned and again the value of lock will be set to TRUE.
so now we have set the value of lock = true and return value r = true so while condition will be true so p2 will be struck in while loop and will not be allowed to get enter into the critical section. By doing it so with the help of test_and_set instruction we have successfully prevented mutual exculusion.

****************Turn variable | Strict alteration method
Process P1 		| Process P2
while(turn !=0)		| while(turn!=1)
critical section	| critical section
turn =1;		| turn = 0;

here assuming initially turn = 0 then P1 will have false while condition then it will get enter into the critical section and then turn = 1 it will be updated.
while the process is in critical condition the process 2 cannot be enter into critical section. As turn = 0 and while conidtion is true for process 2. so it will not be allowed to get enter into critical section. p2 will be only allowed to get into the critical section when turn = 1 and that will be done when process p1 will be terminated.

Remarks:
1) Mutual exclusion is guaranteed as one of the process cannot enter into the critical section when other is in critical section.
2) Progress is not guranteed as for turn = 0 p2 cannot enter in critical section. P2 will be only allowed to get enter into critical section until p1 has not entered in critical section. So p2 is dependent on p2 so progress is not followed here.
3) Bound wait. When P1 has executed it cannot again executed until the p2. and when the p2 has executed it cannot be executed again until p1 has not executed. So no one have is bound wait. each will execute one by one.
4) It is not hardware dependent.

********************Semaphores
There are two types of semaphore. 1st one is counting semaphore that can be from (-infinity to +infinity) and 2nd one is  binary ( 0 or 1)
Semaphore is an integer variable (-infinity to +infinity) which is used in mutual exclusive manners by various concurrent coopertive processes in order to achieve synchronization.
OR in simple words "A semaphore is a value in a designated place in operating system (or kernel) storage that each process can check and then change. Depending on the value that is found, the process can use the resource or will find that it is already in use and must wait for some period before trying again."

P() (Or Down, wait)
V() ( or up, signal, post, release)

we generally do these above operations on semaphores. Every semaphore integer has some meaning. lets understand those meaning with a piece of code

Down(Semaphore S)				| Up (Semaphore S)
{						| {
S.value = S.value - 1;				| S.value = S.value + 1;
if(S.value < 0)					| if(S.value <= 0)
{						| {
put process (PCB) in suspended list		| Select a process from suspended list and wakeup() (add in the ready queue)
}						| }
else						| else
 return;					|  return;
}						| }



Here S is semaphore integer variable and can be denoted by any variable. 
let say S= 3

so lets perform down operation P()
s.value will be decremented to 2 
Is 2 < 0 ---> false ---> entered in to Critical section

now again 

s.value = 1

1 < 0 --> false ---> entered into critical section


again 
s.value = 0 

0 < 0 ---> false --> entered into critical section


so now S= 3 means max 3 process are allowed to enter into critical section

S= 0 means no section is allowed to enter into critical section


so now if S = 0 decrement

s = -1

-1 < 0 ---> true ---> added into block list

and so on

so we can say

S= -1 means --> 1 process will be in block list


Conclusion:
S = -n ---> means n number of processes will be in block list
and S = n means max number of processes are allowed to enter in critical list

where n is not equal to 0;

+ve values are usually termed as successful operations as they would enter in the critical section
-ve values are termed as unsuccessful operations as they wont enter into critical section but block list
--------------

for V() operation means up operation,

Semaphore will be increamented 

on true condition one process from block list will be added to the ready queue


***************Question on semaphore
For S = 10
what will be final value of semaphore on performing 6P, 4V operations?
10-6+4 = 8

S = 8

6P means 6 down, and 4V means for up

For S= 17 
5P, 3V, 1P operation

S = 17-5+3-1 = 14 

S= 14


**************Binary Semaphores
Down()						| Up (Semaphore S){
{						| if(suspended list is empty) {
if(S.value==1)					| S.value = 1;}
						| else
{						| {
S.value=0;					| Select a process from suspended list and wakeup() (add in the ready queue)}
}						| }
else{						| 
 Block this process and add 			| 
suspended list					|
}						| 
}


Example

p1			| P2
			|
Down(S)			| Down(s)
			|
Critical Section	| critical section	
			|
Up(S)			| up(s)


let S =1

and let p2 tried to enter into critical section

p2 have to perform down opertion. It will be a successful operation as S=1 it will be updated to S=0 and P2 will be allowed to access the critical section.
Now p1 also tried to access critical section. It will be failed as now S = 0 and for S=0 down operation would block the process and add it into suspended list. 

when p2 will come out it will call up operation which will check the suspeneded list since there is p1 is suspended list it will pick the suspended list will add it in ready queue so that in future it can be run.



*****************Solving consumer producer problem using semaphores*********
********************Producer and consumer example of process synchronization***********
//Consumer Code				| //Producer code
void consumer(){			| 
					| void produce(void){
1) down(full)				| 1) down(empty)
2) down(S)				| 2) down(s)
					| {
{					| 
itemC = Buffer(out);			| 
out = (out+1)mode(n);			| Buffer[in] = itemP
}					| in = (in + 1) modn;
3) up(S)		 		| 
4) up(empty)				| }
					| 3) up(s)
					| 4) up(full)
}					| }


In last time you might have remember that we were achieving the race condition again in which two wrong values were racing to win. So here our main goal is to prevent the mutual exclusion. That is if one process either consumer or producer are in critical section then the other process should not be allowed to enter in critical section otherwise race condition will be achieved.
Here you just have a concept of down() and up() function. Down() function down or decrement the value whereas up increment of up the value.

Assume we have 8 rows from index 1 to 7 out of which 3 are filled and rest of 5 are empty. So S=1, Empty =5, Full= 3

IN which is the index of producer code have value 3 means the 4th row. Means producer code is ready to add the 4th element in the stack at index 3 which is forth row.
Let S= 1

producer code runs

1) Empty=5--> Empty= 4

before executing point 2 that is down(s) producer prempt

now consumer code will run

1) Full= 3--> Full=2
2) S=1--> s=0

Critical section here it will consume one value

now before exiting from critical condition 

consumer code prempts

now process will resume producer code from line 2)
here S=0 so operation down(0) will be failed so it will block the producer process and will switched back to consumer code

consumer code will be resumed from line 3

3) S=0 --> S=1
4) empty = 5 --> empty =6


Verifying the output
so initially we have S=1, Empty = 5, Full = 3 ---> 5+3=8

after removing or consuming 1 value we have:
S=1, Empty =6, Full =2 ---> 6+2=8

Successfull


so here as you can see we didnt allowed producer code to run when consumer code was in critical section. In this way we prevented the race condition.

*******************Mutex
What is mutex in operating system? 
The full form of mutex is mutual exclusion object. A mutex (mutual exclusion object) is a program object used in computer programming that allows many program threads to alternately access the same resource, such as access to a file.

**INFO**: When a code is in running state it is called process and when it is in idle condition it is called program


*************Reader-writer problem solution with Binary semaphores

Let say we have a database of a railway station. Here the getting the information about the schedule of the tickets is reading the data and booking ticket and registering your information is writing the data. Following are the operations that we cannot allow at the same time

R-W problem
W-R Problem
W-W problem
R-R No problem

So how can we avoid The above problems using binary semaphores?

int rc = 0; //rc means read count
Semaphore mutex = 1
Semaphore  db = 1

//code for reading data

//code for accessing into DB
1) down(mutex);
2) rc = rc+1;
3) if(rc==1) then down(db);
4) up(mutex);

// critical section
DB

//code after exiting from DB

5) down(mutex)
6) rc= rc-1;
7) if(rc==0)then up(db);
8) up(mutex);


-----------------


// code for writing into db
9) down(db)
DB
10) up(db)


****Case 1:
R-W

Reader comes reader code will execute

1) mutex= 1 --> mutex=0
2) rc = 1;
3) true --> db=1--> db=0
4) mutex = 0 --> mutex= 1

R entered into DB

now for writer:

9) db=0 --> db=0
access denied process will be blocked


***Case 2:
W-R


writer will come first:

9) db=1 --> db=0
W enterd into DB


Reader comes reader code will execute

1) mutex= 1 --> mutex=0
2) rc = 1;
3) true --> db=0--> db=0
process will be blocked db is already 0


***Case 3:
W-W


writer 1 will come first:

9) db=1 --> db=0
W enterd into DB

then writer 2 will come:

9) db=0 --> db=0
process will be blocked db is already 0

****Case 4:
R-W

Reader 1 will come first:

1) mutex= 1 --> mutex=0
2) rc = 1;
3) true --> db=1--> db=0
4) mutex = 0 --> mutex= 1

Reader 1 entered into DB

Now when Reader 2 will come:

1) mutex= 1 --> mutex=0
2) rc = 1 --> rc=2
3) false
4) mutex = 0 --> mutex= 1

Reader 2 also get entered into DB

And here you can enter as many reader as you want


Now exiting the reader 1
5) mutex = 1 --> mutex = 0
6) rc=2 --> rc= 1
7) false;
8) mutex=0 --> mutex=1

1 reader removed




***************Dining philospher problem
Assume a round table having 5 philospher P with 5 forks F from index i 0 to 4
Philospher first think and then pick the left his left fork then right fork and then start eating after eating he place back the left fork and then right fork

There may be a condition of race condition occur when two philospher tries to access the same fork.
so to avoid this we will use Binary Semaphore
S0 --> 1
S1 --> 1
S2 --> 1
S3 --> 1
S4 --> 1

P0 --> S0  S1
P1 --> S1  S2
P2 --> S2  S3
P3 --> S3  S4
P4 --> S4  S0

NOTE: we know that wait, down and P all decrease the value of semaphore and signal, up , post and release increase the value of semaphore.

//Code
1) Thinking();

2) wait(take_fork(Si))
3) wait(take_fork(S(i+1)%N)

4) EAT();

5) signal(put_for(i))
6) signal(put_for(i+1)%N)



Case 1:
Let say P0 comes first
1) thinking
2) S0 =1 --> S0=0
3) S1 =1 --> S1=0

EAT();

during eating let say P1 comes

1) thinking
2) S1 = 0 --> S1 =0
process will be blocked as S1 meand Fork 1 has already taken P1 cannot eat until P0 finish his food.


Case 2:
Let say P0 comes first
1) thinking
2) S0 =1 --> S0=0
3) S1 =1 --> S1=0

EAT();

during eating let say P2 comes

1) thinking
2) S0 =1 --> S2=0
3) S3 =1 --> S3=0

EAT();

both will be able to eat. Here you may think mutual exclusion has break here as two process get access to critical condition. The answer here is that its a special case here note that both processes are independent as forks for P2 are totally independent from forks of p0 so its fine they both can eat at the same time.


Case 3: (Deadlock)
Let say P0 comes first
1) thinking
2) S0 =1 --> S0=0
prempt

Let say P1 comes first
1) thinking
2) S1 =1 --> S2=0
prempt

Let say P2 comes first
1) thinking
2) S2 =1 --> S3=0
prempt

Let say P3 comes first
1) thinking
2) S3 =1 --> S4=0
prempt

Let say P4 comes first
1) thinking
2) S4 =1 --> S0=0
prempt


Now CPU will come to P0 and resumed it from line 3)
as P0 only need fork1 to start eating but fork1 is already in used
3) S1=0 --> S1=0
Process will be blocked as S1 = 0

then it will move to P1
3) S2=0 --> S2=0
Process will be blocked as S2 = 0

so here for all philospher none of them would be able to eat. As all have only 1 fork in their left hand all need one other to start eating. In this case all will remain blocked and this condition is called deadlock condition.

****Removing deadlock
To remove deadlock you have to switch the code of the philospher so that instead of taking left fork first and then right fork he should pick right fork first and then left fork. In doing it so all philospher would be able to eat simulaneously except that modified philospher. He will remain blocked until rest of the 4 do not complete eating. In doing this only one philospher will be blocked not all. So we can prevent deadlock in such conditions.


******************************Chapter 4
****************Deadlock and its necessary conditions
********Deadlock condition*****

Its a condition in which two or more processes wait for happening the event but the event will never happen. 

All processes will remain in hold and wait state for execution and will never be execute

Example:
Suppose you go to the bank for opening an account.
The accountant say deposite some money and then we will open your account.

But you are saying that 1st open my account then I will deposite some amount.

Now you both will remain stuck. He wont open until you pay and you wont pay until he opens your account.

Example 2
P1<--R1 (P1 holding Resource R1)

P2<--R2 (P2 holding resource R2)

But 

P1-->R2 (P1 needs R2 for execution)

P2-->R1(P2 need R1 for execution)

P1 wont release R1 until he get R2 and get executed

And same case for p2 
P2 will not release R2 until he get R1 and get executed

Now both are in never ending wait loop.


Necessary conditions for creating Dead condition:

1) Mutual exclusion (if one process is holding a resource other cannot hold the same resource)
2) No premption (there should not be the case that one process holding a resource prempt itself for any reason. Otherwise the other process waiting for Resource 1 will get that resource and will get executed and deadlock will break this should not be the case)
3) Hold and wait (processes should always remain in that state of hold their resources and waiting for the other resources)
4) Circular wait (Process should form a circular wait. E.g if there are n processes then p1 should be dependant on the resourse of last process that is Pn


***************RAG (Reasource Allocation Graph)
This graph is the best way to represent the state of the system. This graph will tell you that whether the deadlock exists in the process or not.
There are some conventions that you have to keep in mind.

There are two things
process vertex and resource vertex.
process vertex is repesented by Pi  in circle and resource vertex are of two type one is single instance vertex and other is multinstacne vertex.
1) singlex instance (means CPU, moniter) is represented by a dot in a square.
2) multinstance (register in CPU) is represented by multi dots in a square.

Example:
P-->R means process is looking/waiting/requesting for a resource to get executed.
P<--R means process is holding/taken/Assigned a resource.

P1<--R1
|    |
|    |
R2-->P2
its a cycle and deadlock exists here. As P1 holding R1 looking for R2
P2 holding R2 looking for R1. None of them will release their resource and none of them will be able to get a resource to get executed.

Example2:
p1<--R1
P2<--R2
P3-->R2
P3-->R1

there is no deadlock as there is no cycle hold.


NOTE:
In case of single instance resource. The RAG cycle hold always have deadlock.
In case of single instance resource. If there is no RAG cycle then there will be no deadlock.
In case of multi instance resouce. Is there be always a deadlock? Answer is NO.

Finite waiting time for execution of a process called starvation (no matters how long the time is but until unless it is a finite number it will be termed as starvation)

Infinite waiting time for execution of a process called deadlock.


**************Deadlock handling and preventions
There are 4 methods which we can implement to handle the deadlock condition

1) Deadlock ignorance or Ostrich method: This method simply says that ignore the deadlock when it occurs. Like whenever our phone got hanged what we all do? We simply restart our systems. Programmers says that since deadlock is a very very rare condition to occur and if we write code to handle deadlock it will reduce the problem. So simply ignore it. 
2) Deadlock prevention : Since we all know to have a deadlock condition the process must be mutual exclusive (non-shareable), no premption, hold and wait, circular wait. If any one or all of the condition becomes false we can prevent the deadlock. But its practically not a good solution. 	a) Making Mutual exclusion false condition: As we have to make all our system shareable to make mutual exclusion a false condition. 
	b) Making no premtion a false condition: How can we make a device shareable such as printer?.  Like we have to allow all the processes 	  	   to get prempt. We cannot allow premption for all process to make the "no premption" condition false. 
	c) Making hold and wait false: Means we have to provide all necessary resource before the program get executed.
	d) Making circular wait to non circular or acyclic wait: We can prevent this condition by giving numbers to process and allow them only 	   to request for the resource in increase order. Simply means process 3 cannot request process 2 for resource. It can only request to 		   process having number assigned greater than 3.
3) Deadlock avoidance: We can follow algorithms such as Banker's algorithm. In this algo. we check whether putting the system in a condition is   safe or unsafe? like will it cause the deadlock or not.
4) Deadlock detection and recovery: In this method. We detect the deadlock and make possible converies. After applying recovery we again detect deadlock. In case of true we again apply recovery otherwise we simply ignore it.

NOTE: All of above methods except the 1st one reduces the system performance thats why we do not implement that. The only method which we pratically follow is Deadlock ignorance or Ostrich method.


****************Banker's Algorith

Question:

Total: A=10, B=5, C=7 
(No. of resources)
Let A=CPU, B=Memory, C=printer

Processes | Allocation	| Max Need  
	  | A   B   C	| A   B   C	
P1	  | 0   1   0	| 7   5   3		     
P2	  | 2	0   0	| 3   2   2	
P3	  | 3	0   2	| 9   0   2	
P4	  | 2	1   1	| 4   2   2	
P5	  | 0   0   2   | 5   3   3

Here Allocation means how many resources already acquired or in used.
Max need means how much they max need to get executed.

Find the safe sequence (Safe sequence means a sequence in which we run the process and deadlock do not occur)

Solution:
Total given: A=10, B=5, C=7 

Processes | Allocation	| Max Need   | Remaining Need  | Available Resources| Sequence Number
	  | A   B   C	| A   B   C  |  A   B   C      |    A   B   C 	    |   	
P1	  | 0   1   0	| 7   5   3  |  7   2   3      |    3   3   2       |   4th P1
P2	  | 2	0   0	| 3   2   2  |  1   2   2      |    5   3   2       |   1st P2
P3	  | 3	0   2	| 9   0   2  |  6   0   0      |    7   4   3	    |   5th P3
P4	  | 2	1   1	| 4   2   2  |  2   1   1      |    7   4   5	    |   2nd P4
P5	  | 0   0   2   | 5   3   3  |  5   3   1      |    7   5   5	    |   3rd P5
Total     | 7   2   5   | 

Formulas:
Available Resource = total allocated resouces - Total Given resources
Remaining Need = Max Need - Allocated
To execute a process--> available resource >= remaining need

NOTE: Once the a process has been executed then its allocated resources would be free/release so you have to add them in available resource to update the available resource list and so on. And the safe sequence cannot be unique. It can vary.


*******************************Chapter 5
**************************Memory Management and Degree of multiprogramming.

As we all know we have divided our whole storage and termed as different names such as cache, registers, RAM (primary memory) and HDD or SDD  (Secondary memory). Here one the more important memory here is RAM which is primary memory. We have to take two concepts together multiprogamming (accodomodating as many programs in RAM) and memory management (That is taking all the process from the secondary memory and taking their chunks to RAM for its execution)

NOTE: Remember CPU is very fast therefore it is connected with the RAM not with the secondary memory as secondary memories are slow. Moreover, we do not import the whole program in the memory we only import its segment or you can say a block into the memory.

Multiprogramming can only be done when the size of your memory is large. So in order to increase the CPU utilization on in other words preventing the CPU to be in the idle state we have to increase the RAM so that we can import as many programs in the RAM and in case of one program is doing any I/O operation then other can be executed.

CPU utlization = 1-K

where k is the number of processes going for I/O operations.

**************Memory Management techniques
Memory management is one the most important job of an operating system. When ever a process is load in a memory then there is a small portion already allocated to the OS. 
There are several memeory management techniques to manage the process to allocate in the memory which are given below:
Majorly there are two main techniques:
1) Contigous
	a) Fixed partition
	b) Variable Partition
2) Non-Contigous
	a) Paging
	b) Mulilevel paging
	c) Inverted Paging
	d) Segmentation
	e) Segmented Paging

Lets discuss each technique one by one

**Contigous-Fixed Partition (static):
Such partitions were used in old Main frame computers where we define the memory partition in advance.
In this type of partition technique the memory is already partited fixedly. Like assume we have a memory of let say 36MB and we divided them into 4 parts of unequal size of 4, 8, 8, 16 MBs (you can also divide them in equal size) 

let say a process p1 comes of 2MB and OS allocate them in the memory of the prition of 4MB then there will be 2MB of space will be left in this 4MB pritition. Can we allocate this empty space to other process? The answer is NO. In Conitgous process the complete process is allocated. we cannot divide the process in different chunks. 

**Internal fragmentation: The wastage of some space in a partition is called Internal-fragmentation. In this case we have some space in a partition but we cannot allocate it.

**External fragmentation: Let say P1 of 2MB, P2 of 6MB, p3 of 6MB and p4 of 14MB comes and all four process get allocated in 4 paritions separately. Here collectively a total space of 8MB is left as all paritiions have 2MBs of free space. now let say a process of 5 MB comes. Can we accomodate that process? The answer is NO. though we have 8MB of total spaces is free but we cannot allocate it due to contiguous technique we cannot divide a process. This is called external-fragmentation.

Limitations of fixed partition in contigous technique:
1) Internal fragmentation will always there
2) external fragmentation can also happen
3) limitation in degree of multiprogramming (Reason: as if there is only four partition, so max 4 process can only come in the memory. Even if the size of each process coming in the memory partition is smaller than the size of partition. Max 4 processes can come. So we cannot accomodate as many processes we want.)
4) Limit of process size: (If there is a process greater in size than the size of partition then we cannot accomodate it)

Contigous-varialble Partition (Dynamic):
In this technique we do not divide the memory in advance. We create the paritition in real time or we can instantly in the memory when a process arrive. Let say a process of 4MB arrives so we instantly create a partition of 4MB and will allocate it to the process. 
In short the partition will be created on time.

limitations:
1) External fragmentation: let say a 4 process come in a row P1, p2, p3, p4. As size of 2, 4, 8, 6 MBs then 4 partitions will be created equal to the size of each process. Then let say while their execution one of the process get terminate let say P2 process that is of 4MB and let say another process get terminated let say p4 i.e 6MBs now there will two holes or empty spaces will created in the memory at the location of p2 and p4 now we have size left of 10MBs. Now if any 5th process comes having the size of let say 9MB so we cannot accomodate it. Though we have empty space of 10MB but still we cannot accomodate the reason is contigous. This is called external fragmentation.
2) Allocation and deallocation will be complex

pros:
but there will be a scope of degree of multiprogramming. 


**************Contigous: First-fit, next fit, best fit, worst fit algorithm

These are the four algorithm which are used to import the processes in the memory. Lets discuss their working

First fit:
This algorithm simply start scanning from the top of the memory and check if there is any first hole. In case of yes it checks the size enough to store the process. If again yes then it stores the process in the memory. Size of the hole can be equal or greater than the process size. It is very fast as it looks from the first come first serve hole to store the process.

Next-fit:
In the algorithm when a second process comes then it will start scanning after first hole not from the start. And then the hole which will come first having size greater or equal than the size of process it will use that.

Best-fit:
This algorithm is used to store the process in a hole or partition which is exactly or nearly equal to the size of coming process. It completely scans the whole memory and then after finding the best-fit hole it stores the process. Its main goal is to provide minimum internal fragmentation. 
This algorithm is slow as it scans the whole memory first.

Worst-fit: 
This algorithm is opposite of best fit. It also scans the whole memory and find the partition which will give the maximum internal fragmentation. This algorithm is also slow as it scans the whole memory first.


***************Non-contgious: paging and framing
In this technique we can divide our process into different parts and allocate them in different locations in memory. But here we have remember that the holes and spaces in the memory are not always constant they are getting change dynamically so when a new process comes we first check the memory how many holes are there and what are their sizes and then we divide our process. This whole chemistry is very time consuming. So in order to efficiently usage of memory we use the concept of paging and framing.

paging means the division of a process and framing means division of memory.

NOTE: size of frame = size of page

we divide the our process in secondary memory into pages in advance and we also do framing of our memory. Since the size of frame is same as size of each page so allocation will be fast and easier. All the pages of process can be fit any where in any frame in the memory.

let say we have 3 frames left and a process of 2 pages comes then this these two of its pages will be fit in the 2 frames of memory.

***************Differences between Logical Address and Physical Address :
The Logical Address is generated by the CPU while the program is running and the Physical Address is the location inside the main memory.

We always use Local Address size and physical address size in  Bytes/Bits if they are not given in Bytes/Bits then we first have to convert them into Bytes/Bits. 
for example: Local Address Size = 4GB then in Bytes it would be

we know that,
2^30 = 1G
2^2 = 4

So, Local address would be
2^30 + 2^2 = 2^32G

here 32 will be Bits.

**NOTE:
pages are stored in secondary memory
frames are stored in ram, during processing

******************Page table entries
Page table is nothing just a table containing all the information of the page data in frame of the memory.
Why there is a need of page table?
Since we all know that CPU map the logical address on physical address to find the data in the frame.

Following are entries that a page table contains:
Frame no. |     Valid/Invalid			| Protection (R,W,X) 		    | Reference (0/1)   	   	| caching  	  | Dirty
	  |   (means whether that page		| what kind of permission is needed | Least recent value		|  Enable/Disable | Modified or not
	  |  exist in the defined frame or not)	| Either read, write or execute	    | tells Is this data has already 	|		  | 
										    | imported in the memory or not	|
********************Multi level paging
why there is a need of multi level paging?
When the size of page is greater than the size of frame. Then we divide our page into more than one page that is called multi-page.

NOTE: In normal paging process, Every process has its own page Table and we also store these page tables into the frames of the main memory. let say there are 20 processes then there will be 20 process tables representing the information of the processess


*****************Inverted paging
In normal tabel we have a format something like that

page no. | frame no.

every process has its own above mentioned table and all these tables would be store in the memory. So, there is space required to store every page table in the memory. SO there could be a shortage of memory.

SO to solve this problem we introduced a global tabel which contains all the page tables of all processes in a single table and then we store that global tale into the memory instead of storing each page table for each process.


why we call it an inverted table? The reason lies in its format:

frame no. | page no. | process ID


since here as compared to normal table frame no. and page no. are inverted there for it is called inverted table. Here process is find by its frame no. where as in normal table the process was found by page  no.

*************************************************
*******************Thrashing
Thrashing a process related to multiprogramming. In simple words thrashing is the sudden decrease in the process of CPU utilization. This scenerio occurs when default paging (absense of required page in main memory) increases and CPU spends its time to bring the required page from the secondary memory to main memory. This time is also called "Default page service time" (time required to service the page from secondary memory to the main memory).


Our main goal is to increase the degree of multiprogramming and maximize the CPU utilization. To do so we only import the 1st page of process instead of importing the whole page in the memory. But in case CPU demands for the page 2 of the process 1. Then CPU will have to bring the page 2 of process 1 from secondary memory to main memory. In this way CPU will spend time in bringing the default page. This will decrease the CPU utilization. if There will be a case where page of every process is required by the CPU then in such case Default page service time will be maximum and CPU utilization will be lowest this is called thrashing.


Image a graph having Degree of mutliprogramming on x-axis and CPU utilization on y-axis and plot a humb shape graphe increasing, reaching to the maximum altitude and then decreasing to lowest. The decreasing graphs shows thrashing.


****************************Segmentation vs paging
In both segmentation and paging we divide our process and import in the main memory. But the difference is that in paging we divide the process without knowing that what is inside the code. And the size of page is fixed. Whereas in segmentation we divide the process in a sensible way knowing that what is inside the code and in this case the size of the segment can vary.

let say there is a function in a code called add(). In paging there may be a case where this page may be divided in this way that this function get divided. Now our half code will present in the other frame and other code will be present the other frame. This may cause a problem of page default.

In segmentation this add() function will be completely considered as a segment. And there could be several segments depending on the code.

Now as we know CPU generates a logical address and we have to convert this address into  physical address to access the required segment from the secondary memory.

There is also a segment table Which contains a base address and the size of the segment.
There will be labelled a starting point and ending point of the segment in the memory.


*******************************Overlay
Overlay is a technique of adjusting a large process in a memory even when the space in the partition is of small size. 
Let say there is a Memory of 32 MB and there are 4 partitions
8MB, 4MB, 4MB, 16MB

Each program is like a different picture puzzle that needs some of these pieces to run. But, there's a challenge: the puzzle board isn't big enough for all the pictures at once. Now, overlay comes into play. Instead of trying to fit all the pictures on the board at the same time (which might not be possible), the operating system cleverly swaps puzzle pieces in and out. It's like having a few pictures on the board, finishing them, removing those pieces, and putting up new pieces for another set of pictures.
The overlay technique helps in efficiently using the limited puzzle board (memory space). Only the pieces needed for the currently running program are on the board, and when a program is done with its pieces, the operating system swaps them out for the next program's pieces.

****************************Virtual Memory
Virtual memory is simply a technique in which operating system creates an illusion for the user to accomodate the larger process in a memory by using some physical memory that is HDD. This is done by swapping of the pages efficiently by the operating system.
Think RAM as your desk. when you desk will be fully occupied you put some documents which are not in used in the drawer and then put the required objects on to the table. and when there will be a need of the document in the drawer then you bring out the documents from the drawer. Swapping is happening here.

Effective Memory Access Time (EMAT) is a metric used to evaluate the overall time it takes for a computer to access and retrieve information from its memory subsystem. It takes into account the various components that contribute to the total memory access time. The formula for Effective Memory Access Time is often expressed as:


************************Translation lookaside buffer (TLB)
Its basically a cache memory which is faster than RAM but very small in size. Normally if CPU want to execute a process then CPU will look into the page table which is in the Main Memory and then here CPU will get the frame number after knowing the frame number CPU will get the access to the process. 

To make this process faster we keep some of our page tables in the TLB memory. Here CPU will get the frame faster from the TLB than the RAM after accessing the frame number CPU will access the process in the memory. 

But there is a problem which is that if the required page is not present in the TLB then CPU will have to follow the normal procedure to get the process but the normal procedure will be followed after spending time in searchin the required page in TLB memory.

NOTE: CPU generates a logical address which contains page number and page size/offset.

*******************Page replacement algorithm
Since we have already discussed that we do not keep all the processes in the memory we just create an illusion for the programmer by keeping some pages of all the process in the main memory. But what if your main  memory has been filled and some other process wants to cone then we have to replace one page. Which page would be replaced? This is decided by the algorithms there are 4 page replacement algorithms which are as follow:
1) FIFO (First in first out): In this algo the frame which was filled first will be replaced in case of any other process comes.
2) Optimal Page replacement: In this algo the page which is demand most lately in the future will be replaced. Here we have to look forward.
3) Least Recently Used (LRU): In this algo the page which was demand earlier in the past will be replaced. Here we have to look back. It is slower than FIFO.
4) Most Recently Used (MRU): In this algo the page which is most recently ised in the past will be replaced. Here we look back too. It is opposite to LRU.

**Beladys Anomaly: Belady is a scientist and he noticed that in case of FIFO algorithm, on increasing the number of frames in the memory the number of page defaults increases. This is a disadvantage of FIFO algorithm. whereas Common sense says that on increasing number of frames we will be able to bring more processes in the memory so there will be less page default. But in actual this do not happened.

***********************************Chapter 6
******************Disk Architecture
A Hard Disk consists of platters (image platter as a simple round shape plate/CD), surface, read write heads, Every platter have tracks on it, Each track have several sector and in sectors we store data.
Hard disk is a collection of multiple platters. Platter stores data in both sides (upside and downside).
platters rotate about their axis and read/write heads can move back and forth to reach their desired track.

*********Disk Access time
Seek time: Time taken by the read write head to reach the desired track
Rotation time: Time taken by read wirte head to complete one full rotation.
Rotational Latency: Time taken to reach the desired sector. (it is hald of rotational time)
Transfer time: Data to be transfered divided by transfer rate.

************Disk scheduling algorithm
Ultimately our main goal is to reduce the seek time (time required by the read/write to reach the desired track).
We do this by using scheduling algorithms:
FCFS (First come first serve): In this algo we will execute the request as the requests are coming. The adv is that there is no starvation (one req is waiting to exe.) in this algo. But 				performance is not good in some cases. As it do not see the distance it simple move the Read/write headers to the desired track.
SSTF (Shortest seek time first): In this algo header will be move to that track first which is nearest to its current position and in this way it provides the minimum seek time. It may 				provide the worst result in some cases. The disadv of this algo is that we always to check first which is the nearest track from the new current position 				and other disadv is of starvation.
SCAN: This algo also called elevator algo. In this algo our header will move either in the larger direction of track or in smaller direction of track but once it has started let say moving 	in the larger direction then it will not stop until it reaches all the tracks coming in its way till the last largest end track whether the request was not asked for going to the 	end of the track but it will go this is flaw of SCAN. Starvation could be an issue in this algo.
LOOK: LOOK is exactly same as SCAN algo but the difference is that it will not go to the end of the track it will go till the request value in the desired direction. Therefore LOOK is more    	efficient than SCAN as it do not waste time in going till the end of the track.
CSCAN (Circular scan): CSCAN is same as SCAN algo but the difference is that once it has reaches to the end of the track in either direction then it has to return to 0 position. While 			returning back to 0 position it will not accept any request.
CLOOK (Circular look): This algo is same as LOOK but the difference is that once it reaches to the largest request value then it will come back to the lowest track value then it will start listening again to the further request. vice versa for smaller track direction while reaching to the lowest track value after reaching the maximum value, it will not listen to any request.

****************************Chapter 7 File System:
In OS every thing is stored in the form of file. Computer send and receives data in the form of file and talks in terms of file.
While storing these files in the HDD we map our files with the sectors.

***There are following operations which we can perform on files:
Creating
Reading
Writing
Deleting (In deleting all content of the file and metadata of the file both get deleted)
Truncating (In truncating only data is deleted its Attributes/Meta Data/Info about the file remains in the Memory)
Repositioning

***Attributes of files (Meta Data: Data about data or info about data)
Name
Extension Type
Identifier
Location
Size
Modified data/ Created data
Protection/ permission (Either this file is readable, writable or executable)
Encryption

***************Allocation Methods in File system:
Here we will discuss how we will allocate our files in the memory. There are two major methods of allocation:
1) Contigous Allocation
2) Non-Continuous Allocation (Linked list allocation, Indexed Allocation)

Why we use Allocation mehods?
For Efficient disk utilization
Faster access of data

****Contigous Allocation
In contigous Allocation we store the data in continous form we do not divide our data at different location in the memory. Since we all know we first divide our data in several blocks then these blocks will be placed next to each other in case of contigous Allocation. let say we have 2 files each having 5 and 7 blocks respectively. And when we allocate them in the memory then let say File A is located at 67 seat number then its first block will be allocate at 67 other at 68 and so on till 71. And let say file B is allocated at 90 seat number then its first block will be at 90 other at 91 and so on and so forth till 96 as there are seven blocks

Adv:
Easy to implement
Excellent read performance

DisAdv:
Disk will become fragmented (As data need the space as of its number of blocks otherwise it wont be fit in the memory)
Difficult to grow file

************Non-Contigous Allocation: Linked List Allocation
In Non-contigous we are allowed to split our data and scatter in the memory where ever we find space. One block of file can be at 67 seat number let say. And its other block can be at 89. No matter!

Each block contains a pointer which points towards the next block of the file. In this way they form a linked list. We need one block to find other. In Contigous we do not need pointers as we know its next block will be available next to it.

Adv:
No exrernal Fragmentation
File can be increased easily


DisAdv:
Large Seek time (Time req. to find the desired sector/Data)
Random Access/Direct access is difficult (As there is a linked list we have to go through the list to get reach to the desired data block)
Overhead of pointers (Several pointers will be used as one pointer is pointing toward the next pointer)

********************Non-Contigous Allocation: Indexed Allocation
In Non-contigous we are allowed to split our data and scatter in the memory where ever we find space. One block of file can be at 67 seat number let say. And its other block can be at 89. No matter!

In this allocation there is block which contains the index/Pointers of all other blocks of a file. Once we have get this global block which contains all the indexes then we will be easily able to direct access to any block of a that file.

Adv:
Support direct access
No external Fragmentation


DisAdv:
Pointer overhead
Multilevel indexeing


*******************************Chapter 8: Security and Violation
While considering security we have to consider CIA:
Confidentiality: My data should be read by unauthorized user.
Integrity: My data should be modified by unauthorized user.
Availability: My data should be available to authorized user whenever he needs it.

Data breaches happens whenever data travels from one place to another. We cannot prevent breaching but all we can do is to encrypt our data (Converting data into some other form so in case of any unauthorized access hacker cannot understand it)

To prevent the modifications we can use sessions which will be expired after some time.

***********Principle of protection
Programs, users and system should be given enough privilge to perform their tasks.
Domain: Set of access-rights such as R read, W write, X executable.

***********
Threat: Potential security violation of which there is chance to occur.
Attack: An attempt to breach security

*************Standard Security Attacks

Normal
Masquerading: Predenting to be someone, an unauthorized user to escalate priviliges.
Man in the middle attack: Sitting between the dataflow and becoming masquerade as a sender to receiver and viceversa.


*************Linker and Loader
HLL (High level language) --> Compiler (compile code, check errors and syntax and convert code into assembly language that is an object file) --> Linker (receives obj file convert it into and executable file) --> Loader (Exports the file the into the memory) --> Memory (Memory gives access to CPU)



***************************************************************************
